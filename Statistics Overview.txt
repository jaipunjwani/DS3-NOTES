Population vs Samples
Population - every object in a set that is well-defined
Sample - subset of the population selected in some way

BIG QUESTION:How do samples differ from the population? (Say we compute average height of a sample, how does this differ from the actual average of the population?)

Population Mean vs sample mean
Population mean - ACTUAL mean - DETERMINISTIC
Sample mean - varies, and is thus random 

notation: N = sample size
 	Greek letters used for population mean, and roman letters used for sample mean

Random variable - non-deterministic (stochastic)
Deterministic - not influenced by chance

**************************************************************************************************************************************************
Law of large numbers: As sample size increases (assuming sample is being chosen in the right way), the sample mean approaches the population mean
Central limit theorem: (sample mean - population mean) -> Normal (0, sd. deviation / sqrt(N))	
	- the estimator is a random variable that is normally distributed with a mean equal to the population mean and a standard deviation of sigma.
	- sd. deviation / sqrt(N) is known as the standard error
	- standard error tells us how much
	- normal distribution - bell curve 


Standard Deviation - measures the dispersion in the underlying population - indicates the variation in the data from the mean
Standard error captures how much our estimator will move around (how uncertain, on average will our estimator be) due to the CHANCE involved in sampling
Standard error doesn't mean much if the estimator is biased

Two properties of an estimator:
1) Bias - 
2) Variance - the degree to which the estimator jumps around

Hypothesis testing: 
t-value: indicates how many standard deviations we are away 
p-value: probability we observe an xbar - mu knot difference due to chance driven by sampling, assuming null hypothesis is true
null hypothesis: the hypothesis that sample observations result purely from chance (MEANING THERE IS NO RELATION BETWEEN X AND Y)


Intro to Statistical Learning
Chapter 3 - Regression, then lab
Assignment: 

___________________________________________________________________________________________________________________________________________________
6/21
Regression, Prediction, and Classification
terms: X - features, y - outcomes
Capital letters - matrix (multiple column)
lower numbers - scalar/vector(column)

Y = f(X) + e (e = error term)
yi = f(xi) + ei

Example: coin flipping

flipi = p + ei
(p = constant, "success rate")
if outcome =1, then ei = 1-p
else, ei = -p

y = alpha + beta1 xi + ei

minimizes (y-hat - y)^2  #note: squaring drastically affects large values

question: How is error constant calculated? Is it just the maximum error observed in a sample data set
*ei varies per observation i*
sometimes line underestimates or overestimates based on differing error values, so polynomial can be used to fit

with a more accurate regression polynomial, (y-hat - y)^2 [RSS] is smaller

Steps for Linear Regression:
1- define the relationship we are trying to model. What is the outcome? What raw features do we have
	ex: height i = f(age i) + e i
2- define a linear model to approximate this relationship
	general form: height i = alpha + (beta1 age i) + (beta2 (age i)^2) + ... (beta p (age i) ^ p)


example: height as a function of age
squared loss: (H-hat - H)^2 (note: hat indicates that it is the value from the model)
absolute loss: |H-hat - H|

Mean Squared Error (MSE) : Summation(squared loss)/n
Model fit - the model has high fit if the MSE is low and vice versa



Training vs. Testing:
We want to TRAIN the model on the data we initially have, and then TEST the model on new data
Usually we subset our data:
X% for training
Y% for test
X=80 is a common % value


Training vs. Testing in practice:
Choose 20% of observations randomly to be in test set. The 80% go on the training set


R-Squared
RSS - residual sum of squares (sum of all squared losses)
TSS - total variation in outcome  (total sum of squares) : summation(y i - mean(y)^2)
R-Squared: 1 - RSS/TSS : fraction of the variation in the outcome variable captured by the model
	- When R-squared is close to 1, the model is good
	- this means that RSS is relatively small compared to TSS

________________________________________________________________________________________________________________

6/22
Feature types:
Binary features - 0 or 1 (ex: male/female represented by 0,1)
Continuous Features: any real value (ex: price)
Categorical features: Can take one of N values (ex: days of week -> 1-7)

CAVEAT: say week of year -> 1:52 ; week 52 and week 1 are not necessarily far apart (dec. 2015 vs. jan. 2016)

Graphs with binary fetaures usually have a constant shift that applies when the binary is turned on (1). Thus, we have two lines for each group in our category, and are able to distinguish 
through the different Y-intercepts. 

Binary graphs - will have same slope, diff intercepts
Binary graphs + interaction terms - will have diff slope and diff intercepts

R notation: ':' will just put interaction term ; '*' will add in interaction terms and each variable individually

Interaction Term - multiplying two features by each other -> usually in the form of: {continuous feature}*{binary feature}

Scenario: Say we have two raw features, A and B. How amny models can we make?

Wthout interactions, we can make: none, A, B, A+B {4}
W/ interactions, we can make: {4}, A*B, A+B+A*B, A+A*B, B+A*B {8}

with p features, we have 2^(p+1) possible models

Interpreting regression output
t-stats evaluates features one by one.
t-stats evaluate the hypothesis that the coefficient is zero (null-hypothesis)
p-value : P(observing >=coeff, if coeff =0)
t-stats: 1.96 or less means that coeff is close to zero


Parametric vs. non-parametric Regression
Y = f(x) + e
Parametric: f defined by a model we write down with parameters that we have to estimate (alpha, beta, etc)
non-parametric: we directly fit the data




Local averaging - define  a neighborhood region at each of the points, and find the average in that neighborhood

Kernel - a method of assigning higher weight to the points closer to the target point I am tyring to fit
Bandwidth - how wide our kernal (window) is
	trend: larger bandwidth, smoother funcitons because there is more data used to make the averages, and is closer to the overall average with less partitions
	Bandwidth is AKA smoothing parameter
Alternative to bandwidth: K-nearest neighbors


When do we use Parametric or Non-Parametric
- relatively small # of features - non-parametric makes sense because it is feasible to directly fit data
- with many features, (and thus many dimensions), parametric models using interaction terms and polynomials are preferred over non-parametric 



______________________________________________________________________________________________________________________________________________________

6/23

Regularization, Prediction, and Pricing
Given a set of data over 1 year - We might use 10 months to train our model, and 2 months to test our model

Cross-validation method: 5 runs, WE break up our data into 5 parts, and use each part as the testing model, and use that model to predict the remaining 80%

glm() - geeralized linear model ; cv.glm() - will do cross-validation
cv.glm assigns random number to data, and sorts it

time-dimension matters: say we have a year's worth of data of oil supply. we want to predict oil prices for say December. We need to keep our data IN ORDER, with respect to time.
IF time-dimension did not matter, we would want to randomize our data (in this case we use cv.glm to randomize)

We have to specify a class of models (ex: local averaging) in order to use the validation set methodology

The variable selection problem: Given p features, I have 2^p models
Solution 1: Compute best model with 1 features, then with 2 features...till p features (drawback: not usually computationally feasible)
Solution 2: Forward stepwise selection: Start with 0 features, add in feature that improves model the most, then lock in that model and in the next step add in the next feature that improves the model
		- the best model yields the highest R^2, or smallest RSS relative to TSS
		- requires fitting approximately p^2/2 models (much smaller than 2^p models)
Solution 3: Backward stepwise selection
		- start with full model and find the LEAST useful predictor, and repeat each round

Regularization - use all then predictors and throw out the less useful ones in one step

LIMIT of OLS (ordinary least squares) - it tends to overfit data: if we supply OLS many x variables, it tries to fit the data the best by assigning BETA values (non-zero when it minimizes the MSE, and zero when the variable has NO effect)
	- when we give many x variables, it may reduce MSE but might not make a good model. 
	- OLS with more variables (x) may OVERFIT data and thus would not usually yield a good predictor model

Lasso Regression
 - LASSO is a class of models
 - method to reduce variance in our model by having high BETA's 
 - imposes penalty for having high B's (basically checks whether or not it really has an impact)
 - penalty term (LAMBDA) determines how we penalize BETA
 - as we increase penalty parameter, we force more coefficients to zero

RIDGE Regression
 - differs from LASSO in that the penalty function squares the coefficients 
 - this penalizes small coefficients (<1) because when squared, they become closer to zero

R Package: glmnet
- provides ridge() and lasso() functions which we can use on glm 
