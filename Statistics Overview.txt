Population vs Samples
Population - every object in a set that is well-defined
Sample - subset of the population selected in some way

BIG QUESTION:How do samples differ from the population? (Say we compute average height of a sample, how does this differ from the actual average of the population?)

Population Mean vs sample mean
Population mean - ACTUAL mean - DETERMINISTIC
Sample mean - varies, and is thus random 

notation: N = sample size
 	Greek letters used for population mean, and roman letters used for sample mean

Random variable - non-deterministic (stochastic)
Deterministic - not influenced by chance

**************************************************************************************************************************************************
Law of large numbers: As sample size increases (assuming sample is being chosen in the right way), the sample mean approaches the population mean
Central limit theorem: (sample mean - population mean) -> Normal (0, sd. deviation / sqrt(N))	
	- the estimator is a random variable that is normally distributed with a mean equal to the population mean and a standard deviation of sigma.
	- sd. deviation / sqrt(N) is known as the standard error
	- standard error tells us how much
	- normal distribution - bell curve 


Standard Deviation - measures the dispersion in the underlying population - indicates the variation in the data from the mean
Standard error captures how much our estimator will move around (how uncertain, on average will our estimator be) due to the CHANCE involved in sampling
Standard error doesn't mean much if the estimator is biased

Two properties of an estimator:
1) Bias - 
2) Variance - the degree to which the estimator jumps around

Hypothesis testing: 
t-value: indicates how many standard deviations we are away 
p-value: probability we observe an xbar - mu knot difference due to chance driven by sampling, assuming null hypothesis is true
null hypothesis: the hypothesis that sample observations result purely from chance (MEANING THERE IS NO RELATION BETWEEN X AND Y)


Intro to Statistical Learning
Chapter 3 - Regression, then lab
Assignment: 

___________________________________________________________________________________________________________________________________________________
6/21
Regression, Prediction, and Classification
terms: X - features, y - outcomes
Capital letters - matrix (multiple column)
lower numbers - scalar/vector(column)

Y = f(X) + e (e = error term)
yi = f(xi) + ei

Example: coin flipping

flipi = p + ei
(p = constant, "success rate")
if outcome =1, then ei = 1-p
else, ei = -p

y = alpha + beta1 xi + ei

minimizes (y-hat - y)^2  #note: squaring drastically affects large values

question: How is error constant calculated? Is it just the maximum error observed in a sample data set
*ei varies per observation i*
sometimes line underestimates or overestimates based on differing error values, so polynomial can be used to fit

with a more accurate regression polynomial, (y-hat - y)^2 [RSS] is smaller

Steps for Linear Regression:
1- define the relationship we are trying to model. What is the outcome? What raw features do we have
	ex: height i = f(age i) + e i
2- define a linear model to approximate this relationship
	general form: height i = alpha + (beta1 age i) + (beta2 (age i)^2) + ... (beta p (age i) ^ p)


example: height as a function of age
squared loss: (H-hat - H)^2 (note: hat indicates that it is the value from the model)
absolute loss: |H-hat - H|

Mean Squared Error (MSE) : Summation(squared loss)/n
Model fit - the model has high fit if the MSE is low and vice versa



Training vs. Testing:
We want to TRAIN the model on the data we initially have, and then TEST the model on new data
Usually we subset our data:
X% for training
Y% for test
X=80 is a common % value


Training vs. Testing in practice:
Choose 20% of observations randomly to be in test set. The 80% go on the training set


R-Squared
RSS - residual sum of squares (sum of all squared losses)
TSS - total variation in outcome  (total sum of squares) : summation(y i - mean(y)^2)
R-Squared: 1 - RSS/TSS : fraction of the variation in the outcome variable captured by the model
	- When R-squared is close to 1, the model is good
	- this means that RSS is relatively small compared to TSS
